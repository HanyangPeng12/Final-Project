---
title: "Stat 378 Final Project Matropolis-Hasting Alagorithm"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Report on Matropolis-Hasting Alagorithm

## Problem 1

### Introduction

In order to understand a certain distribution $p(x)$, the idea of MCMC is to simulate a Markov chain whose equilibrium distribution is $p(x)$, and metropolis-hasting algorithm provides a way to generate the Markov chain. Giving a start value, and with the help of proposal function, M-H produce a new proposed value based only on its previous value in the Markov chain and decide whether to accept the new value by the acceptance rate. If accepted, the chain moves to the new value, and if not, the chain stays with the old value. By repeating these steps, M-H could generate a chain to simulate the distribution.

### code of algorithm

Below is the metropolis-hasting algorithm sample from a beta distribution $\phi$ with parameters (6,4). We would explain the algorithm and how we implemented it later.
```{r}
mh.beta <- function(iterations, start,shape1, shape2,c) {
  phi.old <- start
  #set the intitial value to be the start value
  draws <- c()
  #save the values of chain in a vector
  phi.update <- function(phi.old, shape1, shape2) {
    phi.new <- rbeta(1, c*phi.old, c*(1-phi.old))
    #generate the new value
    accept.prob <- dbeta(phi.new, shape1 = shape1, shape2=shape2)/dbeta(phi.old,shape1 = shape1, shape2 = shape2)*
      (dbeta(phi.old,c*phi.new,c*(1-phi.new))/dbeta(phi.new,c*phi.old,c*(1-phi.old)))
    #caculate the acceptance probability of the new value with a correction factor
    if (runif(1) <= accept.prob) 
    {phi.update=phi.new} 
    #if the acceptance probability is larger then accept the new value 
    else 
        {phi.update=phi.old}
  }
  # if the acceptance probability is lower then stay with the old value 
  for (i in 1:iterations) {
    draws[i] <- phi.old <- phi.update(phi.old, shape1 = shape1,
                                          shape2 = shape2)
  }
  #give the new value to the chain
  return(draws[1:iterations])
}
```

### code instructions

The algorithm is generally a function to describe the process of M-H. The input of the function would be the times of iterations, which refers to the length of chain we want, the start value, two shape parameters of the beta distribution, and the parameter "c" in the proposal function. 

In the function, we use "phi.old" to save the old value of the chain, use "phi.new" to save the new value generated from the old value, the function we use to generate the new value is called "phi.update", and we save the result of the chain in the vector "draw".

When iterating for the first time, we set the "phi.old" to be the start value. Then, we generate a new value based on the start value by proposal function. As mentioned in the problem, the proposal function is of the form $\phi_{old}| \phi_{new} \sim  Beta(c*\phi_{old},c*(1-\phi_{old}))$, which means the new value is generated by producing a random number from the given beta distribution. We notice that, in the beta distribution of the proposal function, the shape parameters are determined by a variable $c$ and the old value of $\phi$, that's just how old value would generate a new value in M-H algorithm.

### proposal function and criterion for accepting

After generating the new value, we need to decide whether to accept the new value.

Towards a normally distributed proposal, the criterion for accepting or rejecting the new value is as follow: (we denote $x$ as old value, and $x^*$ as the new value)  
1. If $p(x^*) \geq p(x)$, that means $p(x)$ has high density near $x^*$, and will be accepted as the new value in the chain.  
2. If $p(x^*) < p(x)$, that indicates that $p(x)$ has low density near $x^*$. In that case, the new value may still be accepted, but only randomly, and with a probability $\frac{p(x^*)}{p(x)}$.

However, we must notice that beta distribution is not symmetric, which means the proposal function needs a correction factor to suit the reversibility condition of Markov chain, that is the transition probability from $x^{(t)}$ to $x^{(t-1)}$ should be equal to that from $x^{(t-1)}$ to $x^{(t)}$.

As a result, we have to introduce a correction factor, which is defined as $C=\frac{q(x^{(t-1)} | x^*) }{q(x^* | x^{(t-1)})}$, where $q$ refers to the density function of each conditional distribution. Therefore, we could calculate the acceptance rate $\alpha$ by $\alpha = \text{min} \left (1,\frac{p(x^*)}{p(x)} \times C\right)$.

Taking the asymmetrical proposal function into consideration, we draw a random number $u$ from distribution $Unif(0,1)$ each time to implement such process, and the criterion for accepting or rejecting the new value is changed into the following criteria:   
1. If $u \leq \alpha$, we accept the new value.  
2. If $u>\alpha$ the new value would be rejected and the chain would stay with the old value. 

Finally, we save the value into the chain vector, and update the value of $\phi$. In further iterations, we repeat this process until we get a chain with desired length.

## Problem 2
The trace plot, autocorrelation plot, and histogram of the draws are as below. 
```{r}
draws<-mh.beta(10000, start = runif(1),shape1 = 6, shape2 = 4,c=1)
par(mfrow=c(1,3))  #1 row, 3 columns
 plot(draws)
 acf(draws)
 hist(draws) 
```

The comparison between the histogram of draws and target distribution of Beta(6,4) is as below.
```{r}
x=seq(0,1,0.001)
hist(draws,freq=F,main="Comparing draws with target distribution")
lines(x,dbeta(x,6,4),col="red")
```

```{r}
ks.test(draws,"pbeta",6,4)
```

According to the comparison of histogram and the result of K-S test (the p-value is quite small), the distribution of draws follows the target distribution Beta(6,4) quite well. The histogram shows that the sampler generally have the same distribution as beta distribution with shape parameters (6,4). The result of K-S test shows that we are quite confident that the sampler and the target distribution shares the same distribution, since both the distance and the p-value of K-S test are quite low.

Therefore, we can conclude that the sampler could simulate the target distribution quite accurately.

## Problem 3
Set $c=0.1,2.5,10$ separately, we rerun the M-H algorithm, and have the trace plots, autocorrelation plots and histograms for these three samplers. The related plots are as below.

For $c=0.1$:
```{r}
draws1<-mh.beta(10000, start = runif(1),shape1 = 6, shape2 = 4,c=0.1)
par(mfrow=c(1,3))  #1 row, 3 columns
 plot(draws1)
 acf(draws1)
 hist(draws1,freq=F) 
lines(x,dbeta(x,6,4),col="red")
```

For $c=2.5$:
```{r}
draws2<-mh.beta(10000, start = runif(1),shape1 = 6, shape2 = 4,c=2.5)
par(mfrow=c(1,3))  #1 row, 3 columns
 plot(draws2)
 acf(draws2)
 hist(draws2,freq=F) 
 lines(x,dbeta(x,6,4),col="red")
```

For $c=10$:
```{r}
draws3<-mh.beta(10000, start = runif(1),shape1 = 6, shape2 = 4,c=10)
par(mfrow=c(1,3))  #1 row, 3 columns
 plot(draws3)
 acf(draws3)
 hist(draws3,freq=F) 
 lines(x,dbeta(x,6,4),col="red")
```

We compare these samplers by comparing their acceptance rates. Generally speaking, neither a too high or too low acceptance rates would benefit the M-H algorithm. If the acceptance rates is too high, it indicates that the proposal distribution is too narrow compared with the target distribution. In that case, the chain we get actually is just hanging around at a certain range, which would lead to bad simulation. Meanwhile, if the acceptance rates is too low, we would stay at the same point for most of the time, this would cause bad simulation as well. Thus, we usually take an acceptance rates between 20% and 30% to be optimal.(https://theoreticalecology.wordpress.com/2011/12/09/mcmc-chain-analysis-and-convergence-diagnostics-with-coda-in-r/) 

We define a function to calculate the acceptance rate with $c$ set to different values, the related codes are as below.

```{r}
accept.rate=function(draws){
  j=0
  for(i in 1:(length(draws)-1)){
    if (draws[i]==draws[i+1]){
      j=j
    }
    else{j=j+1}
  }
  return(j/length(draws))
}
accept.rate(draws) #c=1
accept.rate(draws1) #c=0.1
accept.rate(draws2) #c=2.5
accept.rate(draws3) #C=10
```

We notice that as $c$ becomes larger, the acceptance rates also get higher. This is because that for a beta distribution $Beta(\alpha,\beta)$, its variance would be $\frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}$. Thus, when $c$ becomes larger, the two shape parameters in the proposal function simultaneously get larger, which leads to the decrease in the variance of the proposal function. Therefore, larger $c$ would lead to lower variance in the proposal distribution, which means the new value we generate each time would generally be closer to the old value, and the acceptance rates would go higher.

As we mentioned above, an acceptance rates between 20% and 30% would be optimal, thus $c=1$ would be the most appropriate choice for the sampler, and would be most effective sampler drawing from the target distribution. 





